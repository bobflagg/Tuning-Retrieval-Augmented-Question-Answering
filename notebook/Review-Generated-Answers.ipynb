{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c7b4b5-3301-4598-a31e-7dc8add95f8b",
   "metadata": {},
   "source": [
    "# Reviewing Model Generated Answers\n",
    "\n",
    "In this notebook we'll load and review answers generated by both the base and fine-tuned models. We only have enough GPU memory to load one of these models at a time so we'll need to clear memory between different model runs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7754c4-3494-482f-a834-f442b674ecb9",
   "metadata": {},
   "source": [
    "# Set Base Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8a3a82-c2e5-4559-b95b-584da3eb444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"/opt/enrichment/github/Tuning-Retrieval-Augmented-Question-Answering/data\"\n",
    "MODEL_DIRECTORY = \"/opt/enrichment/github/Tuning-Retrieval-Augmented-Question-Answering/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db2acf-ebc0-4770-9c52-8240510ebca2",
   "metadata": {},
   "source": [
    "# Load Train/Test Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bd2437-d430-490c-a0a4-56b6ab3bd15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,050 Train/Test records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>hashID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6</td>\n",
       "      <td>The Vatican described the visit as a \"further ...</td>\n",
       "      <td>Why does the Defence Secretary believe that fu...</td>\n",
       "      <td>The Defence Secretary believes funding defence...</td>\n",
       "      <td>5f9149a396b2a0d1eed4ed2d4cb4401f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold                                            excerpt  \\\n",
       "49     6  The Vatican described the visit as a \"further ...   \n",
       "\n",
       "                                             question  \\\n",
       "49  Why does the Defence Secretary believe that fu...   \n",
       "\n",
       "                                               answer  \\\n",
       "49  The Defence Secretary believes funding defence...   \n",
       "\n",
       "                              hashID  \n",
       "49  5f9149a396b2a0d1eed4ed2d4cb4401f  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = f\"{DATA_DIRECTORY}/train-test-df.csv\"\n",
    "df = pd.read_csv(path, na_filter=False)\n",
    "print(f\"Loaded {df.shape[0]:,d} Train/Test records.\")\n",
    "#print(df.fold.value_counts())\n",
    "df.sample(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226ce19-c112-4bbf-a021-d1f448a9a652",
   "metadata": {},
   "source": [
    "# Load Evaluation Prompt Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d84a00-070c-4787-a6bf-6540f4b37495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carefully read the excerpt below and then provide a clear concise answer to the follow-up question.\n"
     ]
    }
   ],
   "source": [
    "path = f\"{DATA_DIRECTORY}/leval-1-prompt-prefix.txt\"\n",
    "with open(path) as ifp: prefix = ifp.read()\n",
    "\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71529bd4-9c92-4e41-bbb7-771238d7ea91",
   "metadata": {},
   "source": [
    "# Load Model, Tokenizer and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17af4b99-d6e8-4042-8896-2ae40f844d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utilities import display_sample, load_base_model, load_lora\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb8f67-6b87-47fb-a374-08697faaada4",
   "metadata": {},
   "source": [
    "# Clear memory between different model runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274b2dd4-650d-4cfc-8a37-5879225e16d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mip-172-25-5-124    \u001b[m  Wed Oct  4 20:22:59 2023  \u001b[1m\u001b[30m535.54.03\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA A10G     \u001b[m |\u001b[31m 36'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m14746\u001b[m / \u001b[33m23028\u001b[m MB | \u001b[1m\u001b[30mubuntu\u001b[m(\u001b[33m14434M\u001b[m)\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model, generator = None, None, None\n",
    "torch.cuda.empty_cache()\n",
    "!gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfc1d3-3950-43c4-8547-d9fa745f0715",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336bb5e4-ec94-4752-8d18-2e36cfb2d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model, generator = load_base_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f18e05-d1b1-4c53-a1fc-af1322719d44",
   "metadata": {},
   "source": [
    "## Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760d5fa7-40ad-4266-94d4-746a58b04576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c644ca55bf64ddc9b5866eec163b0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 22.19 GiB total capacity; 7.74 GiB already allocated; 60.56 MiB free; 7.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m fold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      2\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_DIRECTORY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Llama-2-7b-qa-level-1-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m tokenizer, model, generator \u001b[38;5;241m=\u001b[39m \u001b[43mload_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/enrichment/github/Tuning-Retrieval-Augmented-Question-Answering/notebook/utilities.py:34\u001b[0m, in \u001b[0;36mload_lora\u001b[0;34m(model_name, directory, pad_token, padding_side, eos_token, model)\u001b[0m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, directory)\n\u001b[1;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m pad_token\n",
      "File \u001b[0;32m/opt/enrichment/miniconda3/envs/argilla/lib/python3.10/site-packages/transformers/modeling_utils.py:2047\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2042\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` or `8-bit` quantized models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2045\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/enrichment/miniconda3/envs/argilla/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/enrichment/miniconda3/envs/argilla/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/enrichment/miniconda3/envs/argilla/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/enrichment/miniconda3/envs/argilla/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/enrichment/miniconda3/envs/argilla/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/enrichment/miniconda3/envs/argilla/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 22.19 GiB total capacity; 7.74 GiB already allocated; 60.56 MiB free; 7.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "fold = 5\n",
    "directory = f\"{MODEL_DIRECTORY}/Llama-2-7b-qa-level-1-{fold:02d}\"\n",
    "tokenizer, model, generator = load_lora(model_name, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe79a87-6642-400f-8f8d-b224d6fa79c9",
   "metadata": {},
   "source": [
    "## Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a853af0-5b21-44bf-9c1f-372f17e808c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.sample(n=1).iloc[0]\n",
    "display_sample(x, prefix, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d6464f-b182-4afe-ad0f-a432ba2ecc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mip-172-25-5-124    \u001b[m  Wed Oct  4 22:43:12 2023  \u001b[1m\u001b[30m535.54.03\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA A10G     \u001b[m |\u001b[31m 35'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m14746\u001b[m / \u001b[33m23028\u001b[m MB | \u001b[1m\u001b[30mubuntu\u001b[m(\u001b[33m14434M\u001b[m)\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e6d329-a127-4271-856b-0cfd39a75ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct  4 22:44:35 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   35C    P0              60W / 300W |  14442MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     27905      C   .../miniconda3/envs/argilla/bin/python    14434MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e6a6e-4558-4ff4-adf0-1c763c8208dd",
   "metadata": {},
   "source": [
    "# Add Base Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd52af67-5cdc-49b8-96c6-2ba565bfd239",
   "metadata": {},
   "source": [
    "## Load model, tokenizer and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3ec13-c3a6-4a83-baef-653262c6c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/enrichment/miniconda3/envs/argilla/bin/huggingface-cli login --token ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366cc779-1cee-4f22-8e57-5dd77173a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.cuda()\n",
    "# Reload the new tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad706e8-d247-4309-b55d-7c3cac1e6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate(x, generator, verbose=False):\n",
    "    start = time.time()        \n",
    "    completion = generator(x.evalPrompt, max_new_tokens=256)[0]['generated_text']\n",
    "    end = time.time()\n",
    "    delta = end - start\n",
    "    if verbose: print(f\"\\nProcessing time = {delta:0.3f} seconds.\")\n",
    "    completion = completion.split(\"%%ANSWER: \")[-1].strip()\n",
    "    return completion\n",
    "x = df.sample(n=1).iloc[0]\n",
    "completion = generate(x, generator, verbose=True)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810cfea3-df20-4d3a-8026-f669cf532a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with tqdm(total=1000) as pbar:\n",
    "    for index, x in df.iterrows():\n",
    "        prediction = generate(x, generator, verbose=False) \n",
    "        predictions.append(prediction)\n",
    "        pbar.update()\n",
    "df[\"basePredictedAnswer\"] = predictions\n",
    "path = f\"/opt/enrichment/github/Tuning-Retrieval-Augmented-Question-Answering/data/eval-qa-df.csv\"\n",
    "df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a9a467-dd8e-4e5e-a5a2-3e82b89deec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "argilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
